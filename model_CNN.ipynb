{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('data/train.csv')\n",
    "data_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>780</td>\n",
       "      <td>780</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1572</td>\n",
       "      <td>792</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2392</td>\n",
       "      <td>820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3196</td>\n",
       "      <td>804</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60482</th>\n",
       "      <td>275</td>\n",
       "      <td>115360</td>\n",
       "      <td>552</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60483</th>\n",
       "      <td>275</td>\n",
       "      <td>115912</td>\n",
       "      <td>552</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60484</th>\n",
       "      <td>275</td>\n",
       "      <td>116464</td>\n",
       "      <td>552</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60485</th>\n",
       "      <td>275</td>\n",
       "      <td>117020</td>\n",
       "      <td>556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60486</th>\n",
       "      <td>275</td>\n",
       "      <td>117572</td>\n",
       "      <td>552</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60487 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    time    x  y\n",
       "0        1       0  800  0\n",
       "1        1     780  780  0\n",
       "2        1    1572  792  0\n",
       "3        1    2392  820  0\n",
       "4        1    3196  804  0\n",
       "...    ...     ...  ... ..\n",
       "60482  275  115360  552  0\n",
       "60483  275  115912  552  0\n",
       "60484  275  116464  552  0\n",
       "60485  275  117020  556  0\n",
       "60486  275  117572  552  0\n",
       "\n",
       "[60487 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_x_threshold = 1100\n",
    "min_x_threshold = 400\n",
    "data_train = data_train[data_train.x < max_x_threshold]\n",
    "data_train = data_train[data_train.x > min_x_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.index = list(range(len(data_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_id = 200\n",
    "train = data_train[data_train.id < split_id]\n",
    "test = data_train[data_train.id >= split_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train len:  166\n",
      "Test len:  63\n"
     ]
    }
   ],
   "source": [
    "print('Train len: ', len(train.id.unique()))\n",
    "print('Test len: ', len(test.id.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes_regions = {}\n",
    "c = 1\n",
    "for i in range(1,len(train)):\n",
    "    if train.iloc[i-1].y == 0 and train.iloc[i].y == 1:\n",
    "        coord = [i]\n",
    "    if train.iloc[i-1].y == 1 and train.iloc[i].y == 1:\n",
    "        c+=1\n",
    "    if train.iloc[i-1].y == 1 and train.iloc[i].y == 0:\n",
    "        coord.append(i-1)\n",
    "        if train.iloc[i-1].id in spikes_regions.keys():\n",
    "            spikes_regions[train.iloc[i-1].id].append(c)\n",
    "            spikes_regions[train.iloc[i-1].id].append(coord)\n",
    "        else:\n",
    "            spikes_regions[train.iloc[i-1].id] = [c]\n",
    "            spikes_regions[train.iloc[i-1].id] = [coord]\n",
    "        c = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [[322, 335], 14, [722, 735], 11, [1338, 1348]],\n",
       " 2: [[1864, 1872], 10, [1969, 1978], 8, [1990, 1997], 10, [2031, 2040]],\n",
       " 3: [[2047, 2055], 9, [2096, 2104], 10, [2107, 2116], 12, [2133, 2144]],\n",
       " 4: [[2153, 2172], 9, [2198, 2206], 10, [2217, 2226]],\n",
       " 5: [[2343, 2351], 9, [2372, 2380], 12, [2428, 2439], 11, [2451, 2461]],\n",
       " 6: [[2484, 2494], 11, [2541, 2551], 12, [2568, 2579], 10, [2620, 2629]],\n",
       " 7: [[2653, 2664], 10, [2674, 2683], 11, [2738, 2748]],\n",
       " 10: [[2799, 2806], 22, [2817, 2838], 6, [2842, 2847]],\n",
       " 12: [[2871, 2880], 12, [2896, 2907], 11, [2964, 2974]],\n",
       " 13: [[2986, 2994], 8, [3122, 3129], 18, [3139, 3156]],\n",
       " 14: [[3195, 3202], 14, [3216, 3229], 8, [3395, 3402], 10, [3412, 3421]],\n",
       " 15: [[3453, 3460], 7, [3465, 3471], 8, [3526, 3533], 9, [3599, 3607]],\n",
       " 16: [[3620, 3630], 9, [3658, 3666], 7, [3699, 3705], 8, [3758, 3765]],\n",
       " 17: [[3829, 3837], 8, [3853, 3860], 13, [3874, 3886], 17, [4001, 4017]],\n",
       " 19: [[4065, 4074], 8, [4119, 4126], 14, [4249, 4262]],\n",
       " 20: [[4852, 4859], 8, [4962, 4969], 6, [5120, 5125], 8, [5240, 5247]],\n",
       " 21: [[5277, 5284], 14, [5391, 5404], 13, [5562, 5574], 12, [5590, 5601]],\n",
       " 22: [[6477, 6489], 7, [6602, 6608], 8, [6616, 6623], 8, [6632, 6639]],\n",
       " 23: [[7161, 7173], 12, [7250, 7261], 10, [7274, 7283], 14, [7352, 7365]],\n",
       " 24: [[7481, 7491],\n",
       "  11,\n",
       "  [7505, 7515],\n",
       "  12,\n",
       "  [7548, 7559],\n",
       "  8,\n",
       "  [7657, 7664],\n",
       "  8,\n",
       "  [7667, 7674]],\n",
       " 25: [[7923, 7935], 17, [7961, 7977], 11, [8081, 8091]],\n",
       " 27: [[8124, 8132], 8, [8243, 8250], 36, [8356, 8391]],\n",
       " 29: [[8399, 8406],\n",
       "  9,\n",
       "  [8408, 8416],\n",
       "  8,\n",
       "  [8443, 8450],\n",
       "  15,\n",
       "  [8462, 8476],\n",
       "  7,\n",
       "  [8481, 8487]],\n",
       " 30: [[8513, 8525], 9, [8545, 8553], 13, [8608, 8620]],\n",
       " 31: [[8651, 8658], 9, [8673, 8681], 8, [8690, 8697], 21, [8705, 8725]],\n",
       " 32: [[8736, 8744], 8, [8751, 8758], 9, [8767, 8775], 7, [8815, 8821]],\n",
       " 33: [[8853, 8860], 9, [8885, 8893], 15, [8927, 8941], 17, [8943, 8959]],\n",
       " 34: [[8969, 8975], 15, [8983, 8997]],\n",
       " 35: [[9013, 9026], 21, [9031, 9051]],\n",
       " 36: [[9095, 9102],\n",
       "  10,\n",
       "  [9111, 9120],\n",
       "  7,\n",
       "  [9128, 9134],\n",
       "  14,\n",
       "  [9193, 9206],\n",
       "  8,\n",
       "  [9210, 9217]],\n",
       " 38: [[9252, 9263], 17, [9269, 9285]],\n",
       " 39: [[9294, 9307], 23, [9320, 9342], 10, [9347, 9356]],\n",
       " 40: [[9366, 9382], 13, [9387, 9399]],\n",
       " 41: [[9449, 9464], 9, [9512, 9520], 27, [9526, 9552]],\n",
       " 42: [[9602, 9610], 13, [9620, 9632], 10, [9650, 9659]],\n",
       " 43: [[9681, 9693], 7, [9697, 9703], 17, [9740, 9756]],\n",
       " 44: [[9765, 9773], 7, [9789, 9795], 12, [9844, 9855], 10, [9868, 9877]],\n",
       " 45: [[9913, 9922], 16, [9971, 9986], 15, [9988, 10002]],\n",
       " 46: [[10052, 10061], 7, [10146, 10152], 14, [10169, 10182]],\n",
       " 47: [[10192, 10204],\n",
       "  7,\n",
       "  [10206, 10212],\n",
       "  18,\n",
       "  [10216, 10233],\n",
       "  9,\n",
       "  [10237, 10245]],\n",
       " 48: [[10301, 10309], 9, [10312, 10320], 19, [10325, 10343]],\n",
       " 49: [[10359, 10370], 13, [10380, 10392], 11, [10407, 10417]],\n",
       " 51: [[10466, 10477], 10, [10486, 10495], 15, [10502, 10516]],\n",
       " 52: [[10544, 10558], 18, [10584, 10601], 8, [10605, 10612]],\n",
       " 53: [[10640, 10650],\n",
       "  10,\n",
       "  [10654, 10663],\n",
       "  10,\n",
       "  [10676, 10685],\n",
       "  10,\n",
       "  [10688, 10697]],\n",
       " 54: [[10727, 10743], 13, [10747, 10759], 12, [10809, 10820]],\n",
       " 55: [[10893, 10902],\n",
       "  10,\n",
       "  [10907, 10916],\n",
       "  12,\n",
       "  [10932, 10943],\n",
       "  9,\n",
       "  [10946, 10954],\n",
       "  21,\n",
       "  [10959, 10979]],\n",
       " 57: [[11040, 11050],\n",
       "  15,\n",
       "  [11061, 11075],\n",
       "  13,\n",
       "  [11087, 11099],\n",
       "  8,\n",
       "  [11103, 11110]],\n",
       " 58: [[11121, 11148]],\n",
       " 59: [[11166, 11175],\n",
       "  15,\n",
       "  [11204, 11218],\n",
       "  26,\n",
       "  [11224, 11249],\n",
       "  9,\n",
       "  [11251, 11259]],\n",
       " 60: [[11270, 11285],\n",
       "  11,\n",
       "  [11306, 11316],\n",
       "  13,\n",
       "  [11356, 11368],\n",
       "  13,\n",
       "  [11419, 11431]],\n",
       " 61: [[11440, 11451],\n",
       "  11,\n",
       "  [11477, 11487],\n",
       "  16,\n",
       "  [11562, 11577],\n",
       "  14,\n",
       "  [11590, 11603]],\n",
       " 62: [[11669, 11677],\n",
       "  11,\n",
       "  [11685, 11695],\n",
       "  10,\n",
       "  [11720, 11729],\n",
       "  14,\n",
       "  [11743, 11756],\n",
       "  12,\n",
       "  [11760, 11771],\n",
       "  10,\n",
       "  [11778, 11787]],\n",
       " 63: [[11824, 11835],\n",
       "  13,\n",
       "  [11849, 11861],\n",
       "  12,\n",
       "  [11872, 11883],\n",
       "  12,\n",
       "  [11912, 11923]],\n",
       " 64: [[11951, 11960],\n",
       "  12,\n",
       "  [11988, 11999],\n",
       "  13,\n",
       "  [12003, 12015],\n",
       "  13,\n",
       "  [12019, 12031]],\n",
       " 65: [[12045, 12054], 12, [12057, 12068], 14, [12114, 12127]],\n",
       " 66: [[12137, 12149],\n",
       "  9,\n",
       "  [12162, 12170],\n",
       "  16,\n",
       "  [12192, 12207],\n",
       "  11,\n",
       "  [12213, 12223]],\n",
       " 67: [[12249, 12259], 20, [12265, 12284], 14, [12300, 12313]],\n",
       " 68: [[12344, 12354],\n",
       "  10,\n",
       "  [12366, 12375],\n",
       "  10,\n",
       "  [12377, 12386],\n",
       "  10,\n",
       "  [12396, 12405],\n",
       "  12,\n",
       "  [12410, 12421]],\n",
       " 69: [[12448, 12456], 19, [12474, 12492], 23, [12541, 12563]],\n",
       " 70: [[12581, 12591], 21, [12593, 12613], 11, [12640, 12650]],\n",
       " 71: [[12766, 12777],\n",
       "  15,\n",
       "  [12790, 12804],\n",
       "  10,\n",
       "  [12814, 12823],\n",
       "  11,\n",
       "  [12879, 12889]],\n",
       " 72: [[12898, 12907],\n",
       "  13,\n",
       "  [12914, 12926],\n",
       "  14,\n",
       "  [12939, 12952],\n",
       "  13,\n",
       "  [13007, 13019]],\n",
       " 73: [[13046, 13061], 11, [13064, 13074], 13, [13093, 13105]],\n",
       " 74: [[13112, 13128], 10, [13135, 13144], 9, [13148, 13156]],\n",
       " 75: [[13172, 13181],\n",
       "  10,\n",
       "  [13184, 13193],\n",
       "  10,\n",
       "  [13236, 13245],\n",
       "  12,\n",
       "  [13265, 13276]],\n",
       " 76: [[13305, 13315],\n",
       "  11,\n",
       "  [13334, 13344],\n",
       "  15,\n",
       "  [13406, 13420],\n",
       "  14,\n",
       "  [13437, 13450]],\n",
       " 77: [[13550, 13562],\n",
       "  9,\n",
       "  [13637, 13645],\n",
       "  13,\n",
       "  [13648, 13660],\n",
       "  29,\n",
       "  [13700, 13728],\n",
       "  10,\n",
       "  [13732, 13741]],\n",
       " 78: [[13751, 13782], 10, [13784, 13793]],\n",
       " 79: [[13834, 13852], 20, [13858, 13877]],\n",
       " 80: [[13921, 13930], 13, [13934, 13946], 10, [13949, 13958]],\n",
       " 82: [[13972, 13977], 8, [14007, 14014], 9, [14043, 14051], 8, [14087, 14094]],\n",
       " 84: [[14106, 14115], 16, [14120, 14135], 21, [14145, 14165]],\n",
       " 85: [[14179, 14185],\n",
       "  10,\n",
       "  [14203, 14212],\n",
       "  9,\n",
       "  [14267, 14275],\n",
       "  11,\n",
       "  [14278, 14288]],\n",
       " 86: [[14315, 14335], 11, [14345, 14355], 11, [14359, 14369]],\n",
       " 87: [[14403, 14415],\n",
       "  7,\n",
       "  [14419, 14425],\n",
       "  10,\n",
       "  [14538, 14547],\n",
       "  11,\n",
       "  [14549, 14559]],\n",
       " 88: [[14573, 14584],\n",
       "  10,\n",
       "  [14638, 14647],\n",
       "  10,\n",
       "  [14656, 14665],\n",
       "  12,\n",
       "  [14673, 14684]],\n",
       " 89: [[14774, 14782],\n",
       "  7,\n",
       "  [14808, 14814],\n",
       "  9,\n",
       "  [14837, 14845],\n",
       "  10,\n",
       "  [14849, 14858]],\n",
       " 90: [[14959, 14969], 9, [15038, 15046], 10, [15115, 15124]],\n",
       " 91: [[15161, 15171],\n",
       "  10,\n",
       "  [15177, 15186],\n",
       "  19,\n",
       "  [15195, 15213],\n",
       "  41,\n",
       "  [15220, 15260]],\n",
       " 92: [[15268, 15273],\n",
       "  8,\n",
       "  [15300, 15307],\n",
       "  12,\n",
       "  [15311, 15322],\n",
       "  9,\n",
       "  [15326, 15334],\n",
       "  9,\n",
       "  [15339, 15347]],\n",
       " 93: [[15378, 15386], 9, [15411, 15419], 11, [15424, 15434]],\n",
       " 96: [[15452, 15460],\n",
       "  16,\n",
       "  [15463, 15478],\n",
       "  35,\n",
       "  [15486, 15520],\n",
       "  17,\n",
       "  [15522, 15538]],\n",
       " 97: [[15546, 15554], 16, [15575, 15590], 11, [15635, 15645]],\n",
       " 99: [[15666, 15677],\n",
       "  12,\n",
       "  [15685, 15696],\n",
       "  12,\n",
       "  [15724, 15735],\n",
       "  14,\n",
       "  [15755, 15768]],\n",
       " 100: [[15782, 15791], 10, [15795, 15804], 22, [15823, 15844]],\n",
       " 101: [[15854, 15863],\n",
       "  11,\n",
       "  [15879, 15889],\n",
       "  13,\n",
       "  [15897, 15909],\n",
       "  17,\n",
       "  [15923, 15939]],\n",
       " 102: [[15955, 15966],\n",
       "  12,\n",
       "  [15973, 15984],\n",
       "  12,\n",
       "  [15993, 16004],\n",
       "  12,\n",
       "  [16012, 16023]],\n",
       " 103: [[16044, 16057],\n",
       "  11,\n",
       "  [16081, 16091],\n",
       "  11,\n",
       "  [16137, 16147],\n",
       "  27,\n",
       "  [16160, 16186]],\n",
       " 104: [[16219, 16232], 9, [16288, 16296], 8, [16312, 16319]],\n",
       " 107: [[16338, 16344],\n",
       "  11,\n",
       "  [16381, 16391],\n",
       "  8,\n",
       "  [16455, 16462],\n",
       "  8,\n",
       "  [16471, 16478]],\n",
       " 108: [[16689, 16696],\n",
       "  9,\n",
       "  [16783, 16791],\n",
       "  9,\n",
       "  [16818, 16826],\n",
       "  11,\n",
       "  [16833, 16843]],\n",
       " 109: [[16857, 16864], 16, [16957, 16972], 8, [17054, 17061]],\n",
       " 110: [[17094, 17100],\n",
       "  6,\n",
       "  [17124, 17129],\n",
       "  8,\n",
       "  [17141, 17148],\n",
       "  12,\n",
       "  [17156, 17167]],\n",
       " 111: [[17176, 17181],\n",
       "  8,\n",
       "  [17185, 17192],\n",
       "  8,\n",
       "  [17201, 17208],\n",
       "  8,\n",
       "  [17211, 17218]],\n",
       " 112: [[17418, 17430], 18, [17437, 17454]],\n",
       " 113: [[17471, 17479],\n",
       "  11,\n",
       "  [17526, 17536],\n",
       "  11,\n",
       "  [17569, 17579],\n",
       "  10,\n",
       "  [17620, 17629]],\n",
       " 115: [[17644, 17650],\n",
       "  9,\n",
       "  [17689, 17697],\n",
       "  7,\n",
       "  [17800, 17806],\n",
       "  8,\n",
       "  [17816, 17823]],\n",
       " 116: [[17845, 17857],\n",
       "  10,\n",
       "  [17885, 17894],\n",
       "  11,\n",
       "  [17989, 17999],\n",
       "  9,\n",
       "  [18008, 18016]],\n",
       " 117: [[18102, 18111], 25, [18130, 18154]],\n",
       " 118: [[18182, 18189],\n",
       "  6,\n",
       "  [18194, 18199],\n",
       "  8,\n",
       "  [18211, 18218],\n",
       "  9,\n",
       "  [18238, 18246]],\n",
       " 119: [[18358, 18365], 9, [18376, 18384], 14, [18428, 18441]],\n",
       " 120: [[18465, 18471],\n",
       "  11,\n",
       "  [18531, 18541],\n",
       "  9,\n",
       "  [18550, 18558],\n",
       "  9,\n",
       "  [18745, 18753],\n",
       "  11,\n",
       "  [18758, 18768]],\n",
       " 121: [[18793, 18822]],\n",
       " 122: [[18851, 18861], 11, [18924, 18934], 18, [18959, 18976]],\n",
       " 123: [[19060, 19066],\n",
       "  10,\n",
       "  [19123, 19132],\n",
       "  8,\n",
       "  [19160, 19167],\n",
       "  9,\n",
       "  [19185, 19193]],\n",
       " 124: [[19280, 19289], 13, [19301, 19313], 7, [19402, 19408]],\n",
       " 125: [[19444, 19453],\n",
       "  9,\n",
       "  [19463, 19471],\n",
       "  10,\n",
       "  [19477, 19486],\n",
       "  9,\n",
       "  [19492, 19500]],\n",
       " 126: [[19512, 19524], 7, [19527, 19533]],\n",
       " 127: [[20124, 20129],\n",
       "  7,\n",
       "  [20180, 20186],\n",
       "  12,\n",
       "  [20259, 20270],\n",
       "  7,\n",
       "  [20329, 20335]],\n",
       " 128: [[20347, 20356], 15, [20381, 20395], 9, [20419, 20427]],\n",
       " 129: [[20561, 20578], 8, [20603, 20610], 6, [20652, 20657]],\n",
       " 130: [[20664, 20684], 10, [20693, 20702], 9, [20706, 20714]],\n",
       " 131: [[20722, 20733], 9, [20757, 20765], 7, [20788, 20794]],\n",
       " 132: [[23732, 23739], 13, [23807, 23819], 14, [23826, 23839]],\n",
       " 133: [[23862, 23879], 7, [23885, 23891]],\n",
       " 134: [[23903, 23912], 8, [23967, 23974]],\n",
       " 136: [[24048, 24053],\n",
       "  7,\n",
       "  [24061, 24067],\n",
       "  7,\n",
       "  [24120, 24126],\n",
       "  18,\n",
       "  [24469, 24486]],\n",
       " 137: [[24608, 24614],\n",
       "  7,\n",
       "  [25249, 25255],\n",
       "  7,\n",
       "  [25668, 25674],\n",
       "  7,\n",
       "  [25694, 25700]],\n",
       " 138: [[25951, 25959],\n",
       "  11,\n",
       "  [25972, 25982],\n",
       "  13,\n",
       "  [25988, 26000],\n",
       "  8,\n",
       "  [26003, 26010]],\n",
       " 139: [[26066, 26073],\n",
       "  7,\n",
       "  [26104, 26110],\n",
       "  8,\n",
       "  [26165, 26172],\n",
       "  14,\n",
       "  [26194, 26207]],\n",
       " 140: [[26214, 26221],\n",
       "  10,\n",
       "  [26279, 26288],\n",
       "  7,\n",
       "  [26302, 26308],\n",
       "  7,\n",
       "  [26341, 26347],\n",
       "  8,\n",
       "  [26352, 26359]],\n",
       " 143: [[26393, 26402],\n",
       "  7,\n",
       "  [26406, 26412],\n",
       "  7,\n",
       "  [26416, 26422],\n",
       "  22,\n",
       "  [26436, 26457]],\n",
       " 144: [[26485, 26503], 21, [26517, 26537]],\n",
       " 145: [[26546, 26568], 26, [26574, 26599], 12, [26603, 26614]],\n",
       " 146: [[26732, 26740],\n",
       "  9,\n",
       "  [26756, 26764],\n",
       "  7,\n",
       "  [26796, 26802],\n",
       "  17,\n",
       "  [26823, 26839]],\n",
       " 147: [[26850, 26855],\n",
       "  7,\n",
       "  [26969, 26975],\n",
       "  9,\n",
       "  [26980, 26988],\n",
       "  10,\n",
       "  [26993, 27002]],\n",
       " 148: [[27078, 27085],\n",
       "  9,\n",
       "  [27176, 27184],\n",
       "  12,\n",
       "  [27307, 27318],\n",
       "  10,\n",
       "  [27340, 27349]],\n",
       " 149: [[27416, 27422],\n",
       "  9,\n",
       "  [27432, 27440],\n",
       "  9,\n",
       "  [27521, 27529],\n",
       "  7,\n",
       "  [27661, 27667]],\n",
       " 150: [[27726, 27746]],\n",
       " 151: [[28253, 28265],\n",
       "  8,\n",
       "  [28282, 28289],\n",
       "  11,\n",
       "  [28344, 28354],\n",
       "  12,\n",
       "  [28357, 28368]],\n",
       " 152: [[28387, 28396], 17, [28413, 28429], 11, [28450, 28460]],\n",
       " 153: [[28496, 28502], 18, [28527, 28544], 8, [28609, 28616]],\n",
       " 155: [[28637, 28644],\n",
       "  9,\n",
       "  [28648, 28656],\n",
       "  9,\n",
       "  [28666, 28674],\n",
       "  14,\n",
       "  [28738, 28751]],\n",
       " 156: [[29217, 29224], 8, [29278, 29285], 8, [29333, 29340]],\n",
       " 158: [[29897, 29908],\n",
       "  8,\n",
       "  [30029, 30036],\n",
       "  8,\n",
       "  [30155, 30162],\n",
       "  7,\n",
       "  [30181, 30187]],\n",
       " 159: [[30217, 30223], 8, [30247, 30254], 11, [30582, 30592]],\n",
       " 160: [[30656, 30663], 7, [30670, 30676], 20, [30708, 30727]],\n",
       " 161: [[30752, 30761],\n",
       "  7,\n",
       "  [30768, 30774],\n",
       "  7,\n",
       "  [30823, 30829],\n",
       "  8,\n",
       "  [30836, 30843]],\n",
       " 162: [[30898, 30908], 6, [30915, 30920], 11, [30927, 30937]],\n",
       " 163: [[31160, 31176],\n",
       "  10,\n",
       "  [31263, 31272],\n",
       "  11,\n",
       "  [31280, 31290],\n",
       "  13,\n",
       "  [31292, 31304]],\n",
       " 164: [[32058, 32065],\n",
       "  8,\n",
       "  [32567, 32574],\n",
       "  7,\n",
       "  [32718, 32724],\n",
       "  8,\n",
       "  [33056, 33063]],\n",
       " 167: [[33534, 33543], 9, [33781, 33789], 7, [34040, 34046]],\n",
       " 169: [[34548, 34557], 23, [34571, 34593]],\n",
       " 170: [[34628, 34642], 10, [34668, 34677]],\n",
       " 172: [[34693, 34703],\n",
       "  9,\n",
       "  [34719, 34727],\n",
       "  10,\n",
       "  [34730, 34739],\n",
       "  12,\n",
       "  [34758, 34769]],\n",
       " 173: [[34788, 34796],\n",
       "  12,\n",
       "  [34828, 34839],\n",
       "  10,\n",
       "  [34954, 34963],\n",
       "  7,\n",
       "  [34968, 34974]],\n",
       " 174: [[35057, 35065], 11, [35070, 35080]],\n",
       " 177: [[35363, 35377], 9, [35384, 35392], 6, [35437, 35442]],\n",
       " 178: [[35464, 35471],\n",
       "  10,\n",
       "  [35475, 35484],\n",
       "  10,\n",
       "  [35492, 35501],\n",
       "  17,\n",
       "  [35511, 35527]],\n",
       " 179: [[35547, 35554],\n",
       "  8,\n",
       "  [35560, 35567],\n",
       "  9,\n",
       "  [35577, 35585],\n",
       "  22,\n",
       "  [35625, 35646]],\n",
       " 180: [[35689, 35700], 11, [35746, 35756], 17, [35766, 35782]],\n",
       " 182: [[35823, 35844]],\n",
       " 185: [[35885, 35891],\n",
       "  7,\n",
       "  [35912, 35918],\n",
       "  10,\n",
       "  [35929, 35938],\n",
       "  11,\n",
       "  [36029, 36039]],\n",
       " 186: [[36052, 36075], 9, [36098, 36106]],\n",
       " 187: [[36114, 36128], 8, [36161, 36168], 10, [36197, 36206]],\n",
       " 188: [[36237, 36243],\n",
       "  8,\n",
       "  [36245, 36252],\n",
       "  9,\n",
       "  [36270, 36278],\n",
       "  12,\n",
       "  [36299, 36310]],\n",
       " 189: [[36321, 36333],\n",
       "  9,\n",
       "  [36344, 36352],\n",
       "  23,\n",
       "  [36363, 36385],\n",
       "  28,\n",
       "  [36387, 36414]],\n",
       " 190: [[36429, 36438], 8, [36442, 36449], 23, [36461, 36483]],\n",
       " 191: [[36516, 36522],\n",
       "  11,\n",
       "  [36542, 36552],\n",
       "  10,\n",
       "  [36607, 36616],\n",
       "  9,\n",
       "  [36628, 36636]],\n",
       " 193: [[36648, 36661], 13, [36664, 36676], 8, [36688, 36695]],\n",
       " 194: [[36722, 36732],\n",
       "  8,\n",
       "  [36767, 36774],\n",
       "  8,\n",
       "  [36780, 36787],\n",
       "  12,\n",
       "  [36866, 36877]],\n",
       " 195: [[36922, 36934],\n",
       "  10,\n",
       "  [36952, 36961],\n",
       "  13,\n",
       "  [37012, 37024],\n",
       "  12,\n",
       "  [37039, 37050]],\n",
       " 196: [[37106, 37116], 16, [37140, 37155], 19, [37165, 37183]],\n",
       " 197: [[37248, 37269], 10, [37346, 37355]],\n",
       " 199: [[37398, 37408],\n",
       "  7,\n",
       "  [37424, 37430],\n",
       "  8,\n",
       "  [37467, 37474],\n",
       "  13,\n",
       "  [37537, 37549]]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spikes_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id    time    x  y\n",
      "322   1  272904  716  1\n",
      "323   1  273620  716  1\n",
      "324   1  274340  720  1\n",
      "325   1  275060  720  1\n",
      "326   1  275776  716  1\n",
      "327   1  276484  708  1\n",
      "328   1  277200  716  1\n",
      "329   1  277924  724  1\n",
      "330   1  278656  732  1\n",
      "331   1  279440  784  1\n",
      "332   1  280104  664  1\n",
      "333   1  280808  704  1\n",
      "334   1  281504  696  1\n",
      "335   1  282200  696  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x284c684dca0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "frame = data_train.loc[322:335]\n",
    "print(frame)\n",
    "plt.plot(frame.time, frame.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_data(df, frame_len=10, offset=0, spike_threshold=7):\n",
    "    cnn_data = pd.DataFrame([], columns=['i', 'x', 'y'])\n",
    "    for i in df.id.unique():\n",
    "        indices = df[df.id == i].index\n",
    "        if(len(indices) > frame_len):\n",
    "            for n in range((len(indices)-offset)//frame_len):\n",
    "                frame = df.loc[indices]\n",
    "                frame = frame.iloc[n*frame_len+offset:(n+1)*frame_len+offset]\n",
    "                if(sum(frame.y) >= spike_threshold):\n",
    "                    spike = 1\n",
    "                else:\n",
    "                    spike = 0\n",
    "                cnn_data = pd.concat([cnn_data, pd.DataFrame({'i': [frame.index],\n",
    "                                                          'x': [np.asarray(frame.x).astype('float32')], \n",
    "                                                          'y': np.asarray(spike).astype('float32')})], ignore_index=True)\n",
    "    return cnn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dty...</td>\n",
       "      <td>[800.0, 780.0, 792.0, 820.0, 804.0, 828.0, 916...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Int64Index([10, 11, 12, 13, 14, 15, 16, 17, 18...</td>\n",
       "      <td>[836.0, 792.0, 776.0, 736.0, 724.0, 728.0, 760...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Int64Index([20, 21, 22, 23, 24, 25, 26, 27, 28...</td>\n",
       "      <td>[848.0, 816.0, 784.0, 768.0, 736.0, 740.0, 756...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Int64Index([30, 31, 32, 33, 34, 35, 36, 37, 38...</td>\n",
       "      <td>[748.0, 740.0, 756.0, 772.0, 780.0, 752.0, 752...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Int64Index([40, 41, 42, 43, 44, 45, 46, 47, 48...</td>\n",
       "      <td>[752.0, 720.0, 716.0, 720.0, 728.0, 720.0, 748...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4900</th>\n",
       "      <td>Int64Index([50001, 50002, 50003, 50004, 50005,...</td>\n",
       "      <td>[572.0, 572.0, 568.0, 568.0, 568.0, 560.0, 564...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4901</th>\n",
       "      <td>Int64Index([50011, 50012, 50013, 50014, 50015,...</td>\n",
       "      <td>[556.0, 548.0, 548.0, 552.0, 544.0, 556.0, 608...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4902</th>\n",
       "      <td>Int64Index([50021, 50022, 50023, 50024, 50025,...</td>\n",
       "      <td>[560.0, 556.0, 556.0, 552.0, 556.0, 556.0, 556...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4903</th>\n",
       "      <td>Int64Index([50031, 50032, 50033, 50034, 50035,...</td>\n",
       "      <td>[548.0, 552.0, 552.0, 560.0, 560.0, 556.0, 560...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4904</th>\n",
       "      <td>Int64Index([50041, 50042, 50043, 50044, 50045,...</td>\n",
       "      <td>[556.0, 560.0, 560.0, 556.0, 560.0, 560.0, 552...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4905 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      i  \\\n",
       "0     Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dty...   \n",
       "1     Int64Index([10, 11, 12, 13, 14, 15, 16, 17, 18...   \n",
       "2     Int64Index([20, 21, 22, 23, 24, 25, 26, 27, 28...   \n",
       "3     Int64Index([30, 31, 32, 33, 34, 35, 36, 37, 38...   \n",
       "4     Int64Index([40, 41, 42, 43, 44, 45, 46, 47, 48...   \n",
       "...                                                 ...   \n",
       "4900  Int64Index([50001, 50002, 50003, 50004, 50005,...   \n",
       "4901  Int64Index([50011, 50012, 50013, 50014, 50015,...   \n",
       "4902  Int64Index([50021, 50022, 50023, 50024, 50025,...   \n",
       "4903  Int64Index([50031, 50032, 50033, 50034, 50035,...   \n",
       "4904  Int64Index([50041, 50042, 50043, 50044, 50045,...   \n",
       "\n",
       "                                                      x    y  \n",
       "0     [800.0, 780.0, 792.0, 820.0, 804.0, 828.0, 916...  0.0  \n",
       "1     [836.0, 792.0, 776.0, 736.0, 724.0, 728.0, 760...  0.0  \n",
       "2     [848.0, 816.0, 784.0, 768.0, 736.0, 740.0, 756...  0.0  \n",
       "3     [748.0, 740.0, 756.0, 772.0, 780.0, 752.0, 752...  0.0  \n",
       "4     [752.0, 720.0, 716.0, 720.0, 728.0, 720.0, 748...  0.0  \n",
       "...                                                 ...  ...  \n",
       "4900  [572.0, 572.0, 568.0, 568.0, 568.0, 560.0, 564...  0.0  \n",
       "4901  [556.0, 548.0, 548.0, 552.0, 544.0, 556.0, 608...  1.0  \n",
       "4902  [560.0, 556.0, 556.0, 552.0, 556.0, 556.0, 556...  0.0  \n",
       "4903  [548.0, 552.0, 552.0, 560.0, 560.0, 556.0, 560...  0.0  \n",
       "4904  [556.0, 560.0, 560.0, 556.0, 560.0, 560.0, 552...  0.0  \n",
       "\n",
       "[4905 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_len = 10\n",
    "cnn_data = create_cnn_data(data_train, frame_len=frame_len, offset=0)\n",
    "cnn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([cnn_data.x[1]])\n",
    "for x in cnn_data.x[1:]:\n",
    "    X = np.concatenate((X, [x]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1400611620795107"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cnn_data.y)/len(cnn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], X.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, cnn_data.y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(conv_layers, dense_layers, kernel_size, pool_size, frame_len = 10, layer_size=64, lr=0.001, dropout=False):\n",
    "    model = Sequential()\n",
    "    for c in range(conv_layers):\n",
    "        model.add(layers.Conv1D(layer_size, kernel_size, activation='relu', input_shape=(frame_len, 1)))\n",
    "        model.add(layers.MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(layers.Flatten())\n",
    "    for d in range(dense_layers):\n",
    "        model.add(layers.Dense(layer_size, activation = 'relu'))\n",
    "        if(dropout):\n",
    "            model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer, loss=['binary_crossentropy'], metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(2, 2, 3, 2, layer_size=128, dropout=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 10, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 10, 1), dtype=tf.float32, name='conv1d_21_input'), name='conv1d_21_input', description=\"created by layer 'conv1d_21_input'\"), but it was called on an input with incompatible shape (None, 15, 1).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:855 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:838 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1030 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:380 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:420 call\n        return self._run_internal_graph(\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:556 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:251 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer dense_34 is incompatible with the layer: expected axis -1 of input shape to have value 128 but received input with shape (None, 256)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-cb2676e0a807>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 763\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    764\u001b[0m             *args, **kwds))\n\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3049\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3050\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3051\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3444\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3279\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:855 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:838 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1030 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:380 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:420 call\n        return self._run_internal_graph(\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:556 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\mi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:251 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer dense_34 is incompatible with the layer: expected axis -1 of input shape to have value 128 but received input with shape (None, 256)\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_spikes = len(y_test[y_test == 1.0])\n",
    "y_test.index = list(range(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_test, pred):\n",
    "    m = tf.keras.metrics.BinaryAccuracy()\n",
    "    m.update_state(y_test, pred)\n",
    "    ba = m.result().numpy()\n",
    "    f1 = f1_score(y_test, list(map(lambda x: 0 if x < 0.5 else 1, pred)), average='binary')\n",
    "    return ba, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_cnn(y_test, pred, verbose=0):\n",
    "    if(verbose==1 or verbose==2):\n",
    "        print('Spikes found: ', len(pred[pred > 0.5]))\n",
    "        spikes = pred[y_test[y_test == 1.0].index]\n",
    "        tp_spikes = len(spikes[spikes > 0.5])\n",
    "        print('Found correct: ', tp_spikes)\n",
    "        print('TP rate: ', tp_spikes/total_spikes)\n",
    "    ba, f1 = accuracy(y_test, pred)\n",
    "    if(verbose==2):\n",
    "        print('Accuracy: ', ba)\n",
    "        print('F1 score: ', f1)\n",
    "    return ba, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spikes found:  253\n",
      "Found correct:  163\n",
      "TP rate:  0.6707818930041153\n",
      "Accuracy:  0.89499694\n",
      "F1 score:  0.6572580645161291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.89499694, 0.6572580645161291)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_cnn(y_test, cnn_pred, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cnn(cnn_data):\n",
    "    X = np.array([cnn_data.x[1]])\n",
    "    for x in cnn_data.x[1:]:\n",
    "        X = np.concatenate((X, [x]), axis=0)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "    \n",
    "    return model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cnn(test_df, model, frame_len=10, offset=0):\n",
    "    test_data = test_df.copy()\n",
    "    test_cnn_data = create_cnn_data(test_data, frame_len=frame_len, offset=offset)\n",
    "    test_data['pred'] = np.nan\n",
    "    test_cnn_data['pred'] = predict_cnn(test_cnn_data)\n",
    "    for row in test_cnn_data.iterrows():\n",
    "        for i in row[1].i:\n",
    "            test_data.loc[i, 'pred'] = row[1].pred\n",
    "    return accuracy(test_data.y, test_data.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8788827, 0.624516129032258)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_cnn(test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   frame_len  offset  kernel_size  pooling_size  layers_repeat  layer_size  \\\n",
      "0         10       0            3             2              1         128   \n",
      "\n",
      "   batch_size  epochs  dropout  accuracy        f1 f1 (submitted)  \n",
      "0          20     300    False  0.865238  0.573274           None  \n",
      "----------------------------------------\n",
      "   frame_len  offset  kernel_size  pooling_size  layers_repeat  layer_size  \\\n",
      "0         10       0            3             2              1         128   \n",
      "\n",
      "   batch_size  epochs  dropout  accuracy        f1 f1 (submitted)  \n",
      "0          40     300    False  0.851272  0.487745           None  \n",
      "----------------------------------------\n",
      "   frame_len  offset  kernel_size  pooling_size  layers_repeat  layer_size  \\\n",
      "0         10       0            3             2              1         256   \n",
      "\n",
      "   batch_size  epochs  dropout  accuracy        f1 f1 (submitted)  \n",
      "0          20     300    False  0.862188  0.541315           None  \n",
      "----------------------------------------\n",
      "   frame_len  offset  kernel_size  pooling_size  layers_repeat  layer_size  \\\n",
      "0         10       0            3             2              1         256   \n",
      "\n",
      "   batch_size  epochs  dropout  accuracy        f1 f1 (submitted)  \n",
      "0          40     300    False  0.862188  0.564811           None  \n",
      "----------------------------------------\n",
      "   frame_len  offset  kernel_size  pooling_size  layers_repeat  layer_size  \\\n",
      "0         10       0            3             2              2         128   \n",
      "\n",
      "   batch_size  epochs  dropout  accuracy        f1 f1 (submitted)  \n",
      "0          20     300    False  0.850148  0.476617           None  \n",
      "----------------------------------------\n",
      "   frame_len  offset  kernel_size  pooling_size  layers_repeat  layer_size  \\\n",
      "0         10       0            3             2              2         128   \n",
      "\n",
      "   batch_size  epochs  dropout  accuracy        f1 f1 (submitted)  \n",
      "0          40     300    False  0.861225  0.558202           None  \n",
      "----------------------------------------\n",
      "   frame_len  offset  kernel_size  pooling_size  layers_repeat  layer_size  \\\n",
      "0         10       0            3             2              2         256   \n",
      "\n",
      "   batch_size  epochs  dropout  accuracy        f1 f1 (submitted)  \n",
      "0          20     300    False  0.845493  0.506393           None  \n",
      "----------------------------------------\n",
      "   frame_len  offset  kernel_size  pooling_size  layers_repeat  layer_size  \\\n",
      "0         10       0            3             2              2         256   \n",
      "\n",
      "   batch_size  epochs  dropout  accuracy        f1 f1 (submitted)  \n",
      "0          40     300    False  0.849025  0.543534           None  \n",
      "----------------------------------------\n",
      "   frame_len  offset  kernel_size  pooling_size  layers_repeat  layer_size  \\\n",
      "0         15       0            3             2              2         128   \n",
      "\n",
      "   batch_size  epochs  dropout  accuracy        f1 f1 (submitted)  \n",
      "0          20     300    False  0.809294  0.447429           None  \n",
      "----------------------------------------\n",
      "   frame_len  offset  kernel_size  pooling_size  layers_repeat  layer_size  \\\n",
      "0         15       0            3             2              2         128   \n",
      "\n",
      "   batch_size  epochs  dropout  accuracy        f1 f1 (submitted)  \n",
      "0          40     300    False  0.825106  0.470435           None  \n",
      "----------------------------------------\n",
      "   frame_len  offset  kernel_size  pooling_size  layers_repeat  layer_size  \\\n",
      "0         15       0            3             2              2         256   \n",
      "\n",
      "   batch_size  epochs  dropout  accuracy        f1 f1 (submitted)  \n",
      "0          20     300    False  0.791637  0.482684           None  \n",
      "----------------------------------------\n",
      "   frame_len  offset  kernel_size  pooling_size  layers_repeat  layer_size  \\\n",
      "0         15       0            3             2              2         256   \n",
      "\n",
      "   batch_size  epochs  dropout  accuracy       f1 f1 (submitted)  \n",
      "0          40     300    False   0.82302  0.48815           None  \n",
      "----------------------------------------\n",
      "Wall time: 19min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid = pd.DataFrame([], columns=['frame_len', 'offset',\n",
    "                                 'kernel_size', 'pooling_size', 'layers_repeat', 'layer_size', \n",
    "                                 'batch_size', 'epochs', 'dropout',\n",
    "                                 'accuracy', 'f1', 'f1 (submitted)'])\n",
    "\n",
    "offset = 0\n",
    "kernel_size = 3\n",
    "pooling_size = 2\n",
    "lr = 0.001\n",
    "epochs = 300\n",
    "dropout = False\n",
    "layers_params = {10: [1, 2], 15: [2]}\n",
    "thresholds = {5: 5, 10: 7, 15: 8}\n",
    "\n",
    "for layer in layers_params.items():\n",
    "    frame_len = layer[0]\n",
    "    cnn_data = create_cnn_data(train, frame_len=frame_len, offset=offset, spike_threshold=thresholds[frame_len])\n",
    "    for layers_repeat in layer[1]:\n",
    "        for layer_size in [128, 256]:\n",
    "            for batch_size in [20, 40]:\n",
    "                X = np.array([cnn_data.x[1]])\n",
    "                for x in cnn_data.x[1:]:\n",
    "                    X = np.concatenate((X, [x]), axis=0)\n",
    "                X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "                \n",
    "                model = create_model(layers_repeat, layers_repeat, kernel_size, pooling_size, \n",
    "                                     frame_len=frame_len, layer_size=layer_size, dropout=dropout)\n",
    "                model.fit(X, cnn_data.y, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "                ba, f1 = evaluate_cnn(test, model, frame_len=frame_len, offset=offset)\n",
    "                row = pd.DataFrame({'frame_len': [frame_len], \n",
    "                                    'offset': [offset],\n",
    "                                    'kernel_size': [kernel_size], \n",
    "                                    'pooling_size': [pooling_size], \n",
    "                                    'layers_repeat': [layers_repeat], \n",
    "                                    'layer_size': [layer_size], \n",
    "                                    'batch_size': [batch_size], \n",
    "                                    'epochs': [epochs], \n",
    "                                    'dropout': [dropout],\n",
    "                                    'accuracy': [ba],\n",
    "                                    'f1': [f1], \n",
    "                                    'f1 (submitted)': None})\n",
    "                grid = pd.concat([grid, row], ignore_index=True)\n",
    "                grid.to_csv('grid.csv', index='False')\n",
    "                print(row)\n",
    "                print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame_len</th>\n",
       "      <th>offset</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>pooling_size</th>\n",
       "      <th>layers_repeat</th>\n",
       "      <th>layer_size</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>dropout</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1 (submitted)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>300</td>\n",
       "      <td>False</td>\n",
       "      <td>0.865238</td>\n",
       "      <td>0.573274</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>300</td>\n",
       "      <td>False</td>\n",
       "      <td>0.851272</td>\n",
       "      <td>0.487745</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>300</td>\n",
       "      <td>False</td>\n",
       "      <td>0.862188</td>\n",
       "      <td>0.541315</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>300</td>\n",
       "      <td>False</td>\n",
       "      <td>0.862188</td>\n",
       "      <td>0.564811</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>300</td>\n",
       "      <td>False</td>\n",
       "      <td>0.850148</td>\n",
       "      <td>0.476617</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>300</td>\n",
       "      <td>False</td>\n",
       "      <td>0.861225</td>\n",
       "      <td>0.558202</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>300</td>\n",
       "      <td>False</td>\n",
       "      <td>0.845493</td>\n",
       "      <td>0.506393</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>300</td>\n",
       "      <td>False</td>\n",
       "      <td>0.849025</td>\n",
       "      <td>0.543534</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>300</td>\n",
       "      <td>False</td>\n",
       "      <td>0.809294</td>\n",
       "      <td>0.447429</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>300</td>\n",
       "      <td>False</td>\n",
       "      <td>0.825106</td>\n",
       "      <td>0.470435</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>300</td>\n",
       "      <td>False</td>\n",
       "      <td>0.791637</td>\n",
       "      <td>0.482684</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>300</td>\n",
       "      <td>False</td>\n",
       "      <td>0.823020</td>\n",
       "      <td>0.488150</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   frame_len offset kernel_size pooling_size layers_repeat layer_size  \\\n",
       "0         10      0           3            2             1        128   \n",
       "1         10      0           3            2             1        128   \n",
       "2         10      0           3            2             1        256   \n",
       "3         10      0           3            2             1        256   \n",
       "4         10      0           3            2             2        128   \n",
       "5         10      0           3            2             2        128   \n",
       "6         10      0           3            2             2        256   \n",
       "7         10      0           3            2             2        256   \n",
       "8         15      0           3            2             2        128   \n",
       "9         15      0           3            2             2        128   \n",
       "10        15      0           3            2             2        256   \n",
       "11        15      0           3            2             2        256   \n",
       "\n",
       "   batch_size epochs dropout  accuracy        f1 f1 (submitted)  \n",
       "0          20    300   False  0.865238  0.573274           None  \n",
       "1          40    300   False  0.851272  0.487745           None  \n",
       "2          20    300   False  0.862188  0.541315           None  \n",
       "3          40    300   False  0.862188  0.564811           None  \n",
       "4          20    300   False  0.850148  0.476617           None  \n",
       "5          40    300   False  0.861225  0.558202           None  \n",
       "6          20    300   False  0.845493  0.506393           None  \n",
       "7          40    300   False  0.849025  0.543534           None  \n",
       "8          20    300   False  0.809294  0.447429           None  \n",
       "9          40    300   False  0.825106  0.470435           None  \n",
       "10         20    300   False  0.791637  0.482684           None  \n",
       "11         40    300   False  0.823020  0.488150           None  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df):\n",
    "    df['pred'] = np.nan\n",
    "    \n",
    "    cnn_df = pd.DataFrame([], columns=['i', 'x'])\n",
    "    frame_len = 10\n",
    "    spike_threshold = 7\n",
    "    for i in df.id.unique():\n",
    "        indices = df[df.id == i].index\n",
    "        if(len(indices) > frame_len):\n",
    "            for n in range(len(indices)//frame_len):\n",
    "                frame = df.loc[indices]\n",
    "                frame = frame.iloc[n*frame_len:(n+1)*frame_len]\n",
    "                cnn_df = pd.concat([cnn_df, pd.DataFrame({'i': [frame.index],\n",
    "                                                          'x': [np.asarray(frame.x).astype('float32')],})], \n",
    "                                   ignore_index=True)\n",
    "    \n",
    "    cnn_df['pred'] = predict_cnn(cnn_df)\n",
    "    for row in cnn_df.iterrows():\n",
    "        for i in row[1].i:\n",
    "            df.loc[i, 'pred'] = row[1].pred\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results(model):\n",
    "    df = pd.read_csv('data/test.csv')\n",
    "    pred_df = predict(df)\n",
    "    pred_df.columns = ['id', 'time', 'x', 'y']\n",
    "    pred_df.fillna(0)\n",
    "    pred_df.y = list(map(lambda x: 0 if x < 0.5 else 1, pred_df.y))\n",
    "    pred_df = pred_df.drop(columns=['x'])\n",
    "    pred_df.to_csv('test_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "246/246 [==============================] - 1s 2ms/step - loss: 1.4239 - binary_accuracy: 0.8075\n",
      "Epoch 2/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.4003 - binary_accuracy: 0.8563\n",
      "Epoch 3/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.4074 - binary_accuracy: 0.8573\n",
      "Epoch 4/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3932 - binary_accuracy: 0.8577\n",
      "Epoch 5/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3842 - binary_accuracy: 0.8593\n",
      "Epoch 6/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.4036 - binary_accuracy: 0.8579\n",
      "Epoch 7/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3926 - binary_accuracy: 0.8599\n",
      "Epoch 8/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3879 - binary_accuracy: 0.8599\n",
      "Epoch 9/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3813 - binary_accuracy: 0.8599\n",
      "Epoch 10/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3823 - binary_accuracy: 0.8599\n",
      "Epoch 11/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3752 - binary_accuracy: 0.8599\n",
      "Epoch 12/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3913 - binary_accuracy: 0.8569\n",
      "Epoch 13/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3792 - binary_accuracy: 0.8599\n",
      "Epoch 14/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3734 - binary_accuracy: 0.8599\n",
      "Epoch 15/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3809 - binary_accuracy: 0.8579\n",
      "Epoch 16/400\n",
      "246/246 [==============================] - 1s 2ms/step - loss: 0.3817 - binary_accuracy: 0.8597\n",
      "Epoch 17/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3645 - binary_accuracy: 0.8599\n",
      "Epoch 18/400\n",
      "246/246 [==============================] - 1s 2ms/step - loss: 0.3741 - binary_accuracy: 0.8599\n",
      "Epoch 19/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3692 - binary_accuracy: 0.8599\n",
      "Epoch 20/400\n",
      "246/246 [==============================] - 1s 2ms/step - loss: 0.3669 - binary_accuracy: 0.8599\n",
      "Epoch 21/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3694 - binary_accuracy: 0.8599\n",
      "Epoch 22/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3689 - binary_accuracy: 0.8597\n",
      "Epoch 23/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3702 - binary_accuracy: 0.8599\n",
      "Epoch 24/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3618 - binary_accuracy: 0.8595\n",
      "Epoch 25/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3667 - binary_accuracy: 0.8595\n",
      "Epoch 26/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3622 - binary_accuracy: 0.8599\n",
      "Epoch 27/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3620 - binary_accuracy: 0.8599\n",
      "Epoch 28/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3572 - binary_accuracy: 0.8599\n",
      "Epoch 29/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3893 - binary_accuracy: 0.8599\n",
      "Epoch 30/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3769 - binary_accuracy: 0.8599\n",
      "Epoch 31/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3871 - binary_accuracy: 0.8597\n",
      "Epoch 32/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3842 - binary_accuracy: 0.8599\n",
      "Epoch 33/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3817 - binary_accuracy: 0.8599\n",
      "Epoch 34/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3895 - binary_accuracy: 0.8597\n",
      "Epoch 35/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3819 - binary_accuracy: 0.8599\n",
      "Epoch 36/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3823 - binary_accuracy: 0.8599\n",
      "Epoch 37/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3728 - binary_accuracy: 0.8599\n",
      "Epoch 38/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3796 - binary_accuracy: 0.8599\n",
      "Epoch 39/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3793 - binary_accuracy: 0.8599\n",
      "Epoch 40/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3798 - binary_accuracy: 0.8599\n",
      "Epoch 41/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3749 - binary_accuracy: 0.8597\n",
      "Epoch 42/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3791 - binary_accuracy: 0.8599\n",
      "Epoch 43/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3639 - binary_accuracy: 0.8599\n",
      "Epoch 44/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3848 - binary_accuracy: 0.8599\n",
      "Epoch 45/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3900 - binary_accuracy: 0.8599\n",
      "Epoch 46/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3846 - binary_accuracy: 0.8581\n",
      "Epoch 47/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3846 - binary_accuracy: 0.8599\n",
      "Epoch 48/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3878 - binary_accuracy: 0.8599\n",
      "Epoch 49/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3862 - binary_accuracy: 0.8599\n",
      "Epoch 50/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3859 - binary_accuracy: 0.8599\n",
      "Epoch 51/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3839 - binary_accuracy: 0.8599\n",
      "Epoch 52/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3814 - binary_accuracy: 0.8599\n",
      "Epoch 53/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3807 - binary_accuracy: 0.8599\n",
      "Epoch 54/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3810 - binary_accuracy: 0.8599\n",
      "Epoch 55/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3794 - binary_accuracy: 0.8599\n",
      "Epoch 56/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3749 - binary_accuracy: 0.8599\n",
      "Epoch 57/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3725 - binary_accuracy: 0.8599\n",
      "Epoch 58/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3738 - binary_accuracy: 0.8599\n",
      "Epoch 59/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3718 - binary_accuracy: 0.8599\n",
      "Epoch 60/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3671 - binary_accuracy: 0.8599\n",
      "Epoch 61/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3696 - binary_accuracy: 0.8599\n",
      "Epoch 62/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3645 - binary_accuracy: 0.8597\n",
      "Epoch 63/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3698 - binary_accuracy: 0.8599\n",
      "Epoch 64/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3697 - binary_accuracy: 0.8599\n",
      "Epoch 65/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3575 - binary_accuracy: 0.8599\n",
      "Epoch 66/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3647 - binary_accuracy: 0.8599\n",
      "Epoch 67/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3601 - binary_accuracy: 0.8599\n",
      "Epoch 68/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3663 - binary_accuracy: 0.8589\n",
      "Epoch 69/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3423 - binary_accuracy: 0.8599\n",
      "Epoch 70/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3357 - binary_accuracy: 0.8599\n",
      "Epoch 71/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3347 - binary_accuracy: 0.8599\n",
      "Epoch 72/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3498 - binary_accuracy: 0.8599\n",
      "Epoch 73/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3411 - binary_accuracy: 0.8593\n",
      "Epoch 74/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3310 - binary_accuracy: 0.8595\n",
      "Epoch 75/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3267 - binary_accuracy: 0.8597\n",
      "Epoch 76/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3134 - binary_accuracy: 0.8685\n",
      "Epoch 77/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.3113 - binary_accuracy: 0.8693\n",
      "Epoch 78/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2937 - binary_accuracy: 0.8746\n",
      "Epoch 79/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2740 - binary_accuracy: 0.8895\n",
      "Epoch 80/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2696 - binary_accuracy: 0.8905\n",
      "Epoch 81/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2667 - binary_accuracy: 0.8934\n",
      "Epoch 82/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2625 - binary_accuracy: 0.8979\n",
      "Epoch 83/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2551 - binary_accuracy: 0.8999\n",
      "Epoch 84/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2334 - binary_accuracy: 0.9091\n",
      "Epoch 85/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2287 - binary_accuracy: 0.9107\n",
      "Epoch 86/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2248 - binary_accuracy: 0.9097\n",
      "Epoch 87/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2250 - binary_accuracy: 0.9097\n",
      "Epoch 88/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2223 - binary_accuracy: 0.9121\n",
      "Epoch 89/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2146 - binary_accuracy: 0.9136\n",
      "Epoch 90/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2181 - binary_accuracy: 0.9125\n",
      "Epoch 91/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2239 - binary_accuracy: 0.9078\n",
      "Epoch 92/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2131 - binary_accuracy: 0.9123\n",
      "Epoch 93/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2129 - binary_accuracy: 0.9129\n",
      "Epoch 94/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2023 - binary_accuracy: 0.9129\n",
      "Epoch 95/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2022 - binary_accuracy: 0.9129\n",
      "Epoch 96/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2041 - binary_accuracy: 0.9148\n",
      "Epoch 97/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2067 - binary_accuracy: 0.9105\n",
      "Epoch 98/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1973 - binary_accuracy: 0.9158\n",
      "Epoch 99/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1965 - binary_accuracy: 0.9178\n",
      "Epoch 100/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2095 - binary_accuracy: 0.9097\n",
      "Epoch 101/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1983 - binary_accuracy: 0.9136\n",
      "Epoch 102/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1966 - binary_accuracy: 0.9160\n",
      "Epoch 103/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2042 - binary_accuracy: 0.9166\n",
      "Epoch 104/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1982 - binary_accuracy: 0.9134\n",
      "Epoch 105/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1899 - binary_accuracy: 0.9180\n",
      "Epoch 106/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1935 - binary_accuracy: 0.9164\n",
      "Epoch 107/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1975 - binary_accuracy: 0.9142\n",
      "Epoch 108/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1946 - binary_accuracy: 0.9142\n",
      "Epoch 109/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1922 - binary_accuracy: 0.9185\n",
      "Epoch 110/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1898 - binary_accuracy: 0.9180\n",
      "Epoch 111/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1914 - binary_accuracy: 0.9174\n",
      "Epoch 112/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1935 - binary_accuracy: 0.9182\n",
      "Epoch 113/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1945 - binary_accuracy: 0.9199\n",
      "Epoch 114/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1924 - binary_accuracy: 0.9164\n",
      "Epoch 115/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1908 - binary_accuracy: 0.9211\n",
      "Epoch 116/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1859 - binary_accuracy: 0.9215\n",
      "Epoch 117/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1904 - binary_accuracy: 0.9158\n",
      "Epoch 118/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1867 - binary_accuracy: 0.9215\n",
      "Epoch 119/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1844 - binary_accuracy: 0.9205\n",
      "Epoch 120/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1861 - binary_accuracy: 0.9197\n",
      "Epoch 121/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1862 - binary_accuracy: 0.9203\n",
      "Epoch 122/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1865 - binary_accuracy: 0.9176\n",
      "Epoch 123/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1957 - binary_accuracy: 0.9189\n",
      "Epoch 124/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1825 - binary_accuracy: 0.9219\n",
      "Epoch 125/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1866 - binary_accuracy: 0.9223\n",
      "Epoch 126/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1869 - binary_accuracy: 0.9213\n",
      "Epoch 127/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1873 - binary_accuracy: 0.9223\n",
      "Epoch 128/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1798 - binary_accuracy: 0.9229\n",
      "Epoch 129/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1835 - binary_accuracy: 0.9215\n",
      "Epoch 130/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1753 - binary_accuracy: 0.9246\n",
      "Epoch 131/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1791 - binary_accuracy: 0.9215\n",
      "Epoch 132/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1812 - binary_accuracy: 0.9219\n",
      "Epoch 133/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1891 - binary_accuracy: 0.9199\n",
      "Epoch 134/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1869 - binary_accuracy: 0.9238\n",
      "Epoch 135/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1811 - binary_accuracy: 0.9242\n",
      "Epoch 136/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1852 - binary_accuracy: 0.9223\n",
      "Epoch 137/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1830 - binary_accuracy: 0.9219\n",
      "Epoch 138/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1814 - binary_accuracy: 0.9211\n",
      "Epoch 139/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1835 - binary_accuracy: 0.9205\n",
      "Epoch 140/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1758 - binary_accuracy: 0.9215\n",
      "Epoch 141/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1809 - binary_accuracy: 0.9221\n",
      "Epoch 142/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1762 - binary_accuracy: 0.9256\n",
      "Epoch 143/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1797 - binary_accuracy: 0.9213\n",
      "Epoch 144/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1875 - binary_accuracy: 0.9231\n",
      "Epoch 145/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1786 - binary_accuracy: 0.9225\n",
      "Epoch 146/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1759 - binary_accuracy: 0.9260\n",
      "Epoch 147/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1865 - binary_accuracy: 0.9201\n",
      "Epoch 148/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1798 - binary_accuracy: 0.9231\n",
      "Epoch 149/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1730 - binary_accuracy: 0.9250\n",
      "Epoch 150/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1811 - binary_accuracy: 0.9238\n",
      "Epoch 151/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1798 - binary_accuracy: 0.9229\n",
      "Epoch 152/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1810 - binary_accuracy: 0.9248\n",
      "Epoch 153/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1822 - binary_accuracy: 0.9221\n",
      "Epoch 154/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1751 - binary_accuracy: 0.9225\n",
      "Epoch 155/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1703 - binary_accuracy: 0.9248\n",
      "Epoch 156/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1775 - binary_accuracy: 0.9227\n",
      "Epoch 157/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1806 - binary_accuracy: 0.9213\n",
      "Epoch 158/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1741 - binary_accuracy: 0.9280\n",
      "Epoch 159/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1742 - binary_accuracy: 0.9244\n",
      "Epoch 160/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1748 - binary_accuracy: 0.9254\n",
      "Epoch 161/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1676 - binary_accuracy: 0.9291\n",
      "Epoch 162/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1754 - binary_accuracy: 0.9254\n",
      "Epoch 163/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1696 - binary_accuracy: 0.9297\n",
      "Epoch 164/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1708 - binary_accuracy: 0.9254\n",
      "Epoch 165/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1733 - binary_accuracy: 0.9250\n",
      "Epoch 166/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1714 - binary_accuracy: 0.9278\n",
      "Epoch 167/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1677 - binary_accuracy: 0.9278\n",
      "Epoch 168/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1743 - binary_accuracy: 0.9250\n",
      "Epoch 169/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1710 - binary_accuracy: 0.9238\n",
      "Epoch 170/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1654 - binary_accuracy: 0.9293\n",
      "Epoch 171/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1771 - binary_accuracy: 0.9264\n",
      "Epoch 172/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1701 - binary_accuracy: 0.9286\n",
      "Epoch 173/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1673 - binary_accuracy: 0.9295\n",
      "Epoch 174/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1701 - binary_accuracy: 0.9274\n",
      "Epoch 175/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1719 - binary_accuracy: 0.9288\n",
      "Epoch 176/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1677 - binary_accuracy: 0.9301\n",
      "Epoch 177/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1671 - binary_accuracy: 0.9286\n",
      "Epoch 178/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1632 - binary_accuracy: 0.9272\n",
      "Epoch 179/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1656 - binary_accuracy: 0.9272\n",
      "Epoch 180/400\n",
      "246/246 [==============================] - 1s 2ms/step - loss: 0.1770 - binary_accuracy: 0.9280\n",
      "Epoch 181/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1718 - binary_accuracy: 0.9266\n",
      "Epoch 182/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1634 - binary_accuracy: 0.9291\n",
      "Epoch 183/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1637 - binary_accuracy: 0.9303\n",
      "Epoch 184/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1686 - binary_accuracy: 0.9278\n",
      "Epoch 185/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1654 - binary_accuracy: 0.9291\n",
      "Epoch 186/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1658 - binary_accuracy: 0.9313\n",
      "Epoch 187/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1696 - binary_accuracy: 0.9288\n",
      "Epoch 188/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1609 - binary_accuracy: 0.9309\n",
      "Epoch 189/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1588 - binary_accuracy: 0.9327\n",
      "Epoch 190/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1653 - binary_accuracy: 0.9288\n",
      "Epoch 191/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1597 - binary_accuracy: 0.9305\n",
      "Epoch 192/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1672 - binary_accuracy: 0.9303\n",
      "Epoch 193/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1743 - binary_accuracy: 0.9288\n",
      "Epoch 194/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1747 - binary_accuracy: 0.9293\n",
      "Epoch 195/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1603 - binary_accuracy: 0.9297\n",
      "Epoch 196/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1581 - binary_accuracy: 0.9293\n",
      "Epoch 197/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1607 - binary_accuracy: 0.9303\n",
      "Epoch 198/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1675 - binary_accuracy: 0.9278\n",
      "Epoch 199/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1608 - binary_accuracy: 0.9313\n",
      "Epoch 200/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1564 - binary_accuracy: 0.9352\n",
      "Epoch 201/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1621 - binary_accuracy: 0.9325\n",
      "Epoch 202/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1594 - binary_accuracy: 0.9321\n",
      "Epoch 203/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1609 - binary_accuracy: 0.9321\n",
      "Epoch 204/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1540 - binary_accuracy: 0.9333\n",
      "Epoch 205/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1566 - binary_accuracy: 0.9323\n",
      "Epoch 206/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1633 - binary_accuracy: 0.9315\n",
      "Epoch 207/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1586 - binary_accuracy: 0.9329\n",
      "Epoch 208/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1546 - binary_accuracy: 0.9339\n",
      "Epoch 209/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1636 - binary_accuracy: 0.9325\n",
      "Epoch 210/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1530 - binary_accuracy: 0.9325\n",
      "Epoch 211/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1554 - binary_accuracy: 0.9339\n",
      "Epoch 212/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1574 - binary_accuracy: 0.9354\n",
      "Epoch 213/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1523 - binary_accuracy: 0.9350\n",
      "Epoch 214/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1578 - binary_accuracy: 0.9341\n",
      "Epoch 215/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1560 - binary_accuracy: 0.9348\n",
      "Epoch 216/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1544 - binary_accuracy: 0.9323\n",
      "Epoch 217/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1494 - binary_accuracy: 0.9360\n",
      "Epoch 218/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1655 - binary_accuracy: 0.9311\n",
      "Epoch 219/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1528 - binary_accuracy: 0.9344\n",
      "Epoch 220/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1606 - binary_accuracy: 0.9339\n",
      "Epoch 221/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1549 - binary_accuracy: 0.9352\n",
      "Epoch 222/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1554 - binary_accuracy: 0.9366\n",
      "Epoch 223/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1532 - binary_accuracy: 0.9372\n",
      "Epoch 224/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1450 - binary_accuracy: 0.9352\n",
      "Epoch 225/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1537 - binary_accuracy: 0.9335\n",
      "Epoch 226/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1478 - binary_accuracy: 0.9382\n",
      "Epoch 227/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1489 - binary_accuracy: 0.9354\n",
      "Epoch 228/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1513 - binary_accuracy: 0.9366\n",
      "Epoch 229/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1461 - binary_accuracy: 0.9384\n",
      "Epoch 230/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1586 - binary_accuracy: 0.9366\n",
      "Epoch 231/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1487 - binary_accuracy: 0.9358\n",
      "Epoch 232/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1486 - binary_accuracy: 0.9386\n",
      "Epoch 233/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1476 - binary_accuracy: 0.9360\n",
      "Epoch 234/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1488 - binary_accuracy: 0.9366\n",
      "Epoch 235/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1538 - binary_accuracy: 0.9341\n",
      "Epoch 236/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1569 - binary_accuracy: 0.9354\n",
      "Epoch 237/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1447 - binary_accuracy: 0.9384\n",
      "Epoch 238/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1453 - binary_accuracy: 0.9390\n",
      "Epoch 239/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1458 - binary_accuracy: 0.9368\n",
      "Epoch 240/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1515 - binary_accuracy: 0.9358\n",
      "Epoch 241/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1435 - binary_accuracy: 0.9394\n",
      "Epoch 242/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1527 - binary_accuracy: 0.9331\n",
      "Epoch 243/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1467 - binary_accuracy: 0.9384\n",
      "Epoch 244/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1441 - binary_accuracy: 0.9394\n",
      "Epoch 245/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1499 - binary_accuracy: 0.9364\n",
      "Epoch 246/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1421 - binary_accuracy: 0.9394\n",
      "Epoch 247/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1387 - binary_accuracy: 0.9423\n",
      "Epoch 248/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1377 - binary_accuracy: 0.9394\n",
      "Epoch 249/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1484 - binary_accuracy: 0.9358\n",
      "Epoch 250/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1994 - binary_accuracy: 0.9252\n",
      "Epoch 251/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1772 - binary_accuracy: 0.9266\n",
      "Epoch 252/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1586 - binary_accuracy: 0.9331\n",
      "Epoch 253/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1518 - binary_accuracy: 0.9368\n",
      "Epoch 254/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1405 - binary_accuracy: 0.9386\n",
      "Epoch 255/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1533 - binary_accuracy: 0.9356\n",
      "Epoch 256/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1469 - binary_accuracy: 0.9384\n",
      "Epoch 257/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1506 - binary_accuracy: 0.9352\n",
      "Epoch 258/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1455 - binary_accuracy: 0.9374\n",
      "Epoch 259/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1397 - binary_accuracy: 0.9415\n",
      "Epoch 260/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1395 - binary_accuracy: 0.9382\n",
      "Epoch 261/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1338 - binary_accuracy: 0.9433\n",
      "Epoch 262/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1456 - binary_accuracy: 0.9376A: 0s - loss: 0.1605 - binary_accuracy\n",
      "Epoch 263/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1357 - binary_accuracy: 0.9411\n",
      "Epoch 264/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1397 - binary_accuracy: 0.9399\n",
      "Epoch 265/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1414 - binary_accuracy: 0.9405\n",
      "Epoch 266/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1409 - binary_accuracy: 0.9392\n",
      "Epoch 267/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1368 - binary_accuracy: 0.9423\n",
      "Epoch 268/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1359 - binary_accuracy: 0.9443\n",
      "Epoch 269/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1348 - binary_accuracy: 0.9429\n",
      "Epoch 270/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1360 - binary_accuracy: 0.9405\n",
      "Epoch 271/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1356 - binary_accuracy: 0.9427\n",
      "Epoch 272/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1340 - binary_accuracy: 0.9425\n",
      "Epoch 273/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1443 - binary_accuracy: 0.9411\n",
      "Epoch 274/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1296 - binary_accuracy: 0.9456\n",
      "Epoch 275/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1299 - binary_accuracy: 0.9448\n",
      "Epoch 276/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1352 - binary_accuracy: 0.9421\n",
      "Epoch 277/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1329 - binary_accuracy: 0.9452\n",
      "Epoch 278/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1385 - binary_accuracy: 0.9411\n",
      "Epoch 279/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1405 - binary_accuracy: 0.9439\n",
      "Epoch 280/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1340 - binary_accuracy: 0.9433\n",
      "Epoch 281/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1371 - binary_accuracy: 0.9421\n",
      "Epoch 282/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1312 - binary_accuracy: 0.9452\n",
      "Epoch 283/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1328 - binary_accuracy: 0.9439\n",
      "Epoch 284/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1277 - binary_accuracy: 0.9468\n",
      "Epoch 285/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1302 - binary_accuracy: 0.9458\n",
      "Epoch 286/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1288 - binary_accuracy: 0.9458\n",
      "Epoch 287/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1399 - binary_accuracy: 0.9431\n",
      "Epoch 288/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1288 - binary_accuracy: 0.9460\n",
      "Epoch 289/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1319 - binary_accuracy: 0.9423\n",
      "Epoch 290/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1303 - binary_accuracy: 0.9468\n",
      "Epoch 291/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1288 - binary_accuracy: 0.9452\n",
      "Epoch 292/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1269 - binary_accuracy: 0.9472\n",
      "Epoch 293/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2624 - binary_accuracy: 0.9315\n",
      "Epoch 294/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2215 - binary_accuracy: 0.9099\n",
      "Epoch 295/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1908 - binary_accuracy: 0.9221\n",
      "Epoch 296/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1718 - binary_accuracy: 0.9297\n",
      "Epoch 297/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1609 - binary_accuracy: 0.9331\n",
      "Epoch 298/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1512 - binary_accuracy: 0.9364\n",
      "Epoch 299/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1450 - binary_accuracy: 0.9388\n",
      "Epoch 300/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1411 - binary_accuracy: 0.9405\n",
      "Epoch 301/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1414 - binary_accuracy: 0.9419\n",
      "Epoch 302/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1316 - binary_accuracy: 0.9470\n",
      "Epoch 303/400\n",
      "246/246 [==============================] - 1s 2ms/step - loss: 0.1312 - binary_accuracy: 0.9462\n",
      "Epoch 304/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1307 - binary_accuracy: 0.9427\n",
      "Epoch 305/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1241 - binary_accuracy: 0.9472\n",
      "Epoch 306/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1294 - binary_accuracy: 0.9441\n",
      "Epoch 307/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1215 - binary_accuracy: 0.9496\n",
      "Epoch 308/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1227 - binary_accuracy: 0.9476\n",
      "Epoch 309/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1222 - binary_accuracy: 0.9476\n",
      "Epoch 310/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1251 - binary_accuracy: 0.9460\n",
      "Epoch 311/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1185 - binary_accuracy: 0.9517\n",
      "Epoch 312/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1190 - binary_accuracy: 0.9472\n",
      "Epoch 313/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1229 - binary_accuracy: 0.9458\n",
      "Epoch 314/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1376 - binary_accuracy: 0.9415\n",
      "Epoch 315/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1309 - binary_accuracy: 0.9435\n",
      "Epoch 316/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1174 - binary_accuracy: 0.9505\n",
      "Epoch 317/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1323 - binary_accuracy: 0.9413\n",
      "Epoch 318/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1307 - binary_accuracy: 0.9439\n",
      "Epoch 319/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1235 - binary_accuracy: 0.9472\n",
      "Epoch 320/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1173 - binary_accuracy: 0.9509\n",
      "Epoch 321/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1265 - binary_accuracy: 0.9482\n",
      "Epoch 322/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1173 - binary_accuracy: 0.9523\n",
      "Epoch 323/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1193 - binary_accuracy: 0.9480\n",
      "Epoch 324/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1182 - binary_accuracy: 0.9501\n",
      "Epoch 325/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1167 - binary_accuracy: 0.9488\n",
      "Epoch 326/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1289 - binary_accuracy: 0.9478\n",
      "Epoch 327/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1316 - binary_accuracy: 0.9484\n",
      "Epoch 328/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1303 - binary_accuracy: 0.9435\n",
      "Epoch 329/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1266 - binary_accuracy: 0.9445\n",
      "Epoch 330/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1183 - binary_accuracy: 0.9468\n",
      "Epoch 331/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1163 - binary_accuracy: 0.9517\n",
      "Epoch 332/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1223 - binary_accuracy: 0.9492\n",
      "Epoch 333/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1174 - binary_accuracy: 0.9488\n",
      "Epoch 334/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1259 - binary_accuracy: 0.9464\n",
      "Epoch 335/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1152 - binary_accuracy: 0.9533\n",
      "Epoch 336/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1149 - binary_accuracy: 0.9513\n",
      "Epoch 337/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1176 - binary_accuracy: 0.9517\n",
      "Epoch 338/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1153 - binary_accuracy: 0.9527\n",
      "Epoch 339/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1184 - binary_accuracy: 0.9494\n",
      "Epoch 340/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1211 - binary_accuracy: 0.9462\n",
      "Epoch 341/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1469 - binary_accuracy: 0.9415\n",
      "Epoch 342/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1132 - binary_accuracy: 0.9523\n",
      "Epoch 343/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1078 - binary_accuracy: 0.9535\n",
      "Epoch 344/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1097 - binary_accuracy: 0.9541\n",
      "Epoch 345/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1147 - binary_accuracy: 0.9515\n",
      "Epoch 346/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1159 - binary_accuracy: 0.9498\n",
      "Epoch 347/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1122 - binary_accuracy: 0.9523\n",
      "Epoch 348/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1295 - binary_accuracy: 0.9494\n",
      "Epoch 349/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2786 - binary_accuracy: 0.8818\n",
      "Epoch 350/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2366 - binary_accuracy: 0.8979\n",
      "Epoch 351/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2258 - binary_accuracy: 0.9052\n",
      "Epoch 352/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2158 - binary_accuracy: 0.9060\n",
      "Epoch 353/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1773 - binary_accuracy: 0.9252\n",
      "Epoch 354/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1420 - binary_accuracy: 0.9384\n",
      "Epoch 355/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1268 - binary_accuracy: 0.9462\n",
      "Epoch 356/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1238 - binary_accuracy: 0.9468\n",
      "Epoch 357/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1184 - binary_accuracy: 0.9480\n",
      "Epoch 358/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1195 - binary_accuracy: 0.9468\n",
      "Epoch 359/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1228 - binary_accuracy: 0.9494\n",
      "Epoch 360/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1274 - binary_accuracy: 0.9460\n",
      "Epoch 361/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1125 - binary_accuracy: 0.9549\n",
      "Epoch 362/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1122 - binary_accuracy: 0.9517\n",
      "Epoch 363/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1092 - binary_accuracy: 0.9545\n",
      "Epoch 364/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1137 - binary_accuracy: 0.9541\n",
      "Epoch 365/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1221 - binary_accuracy: 0.9492\n",
      "Epoch 366/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1139 - binary_accuracy: 0.9525\n",
      "Epoch 367/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1164 - binary_accuracy: 0.9509\n",
      "Epoch 368/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1093 - binary_accuracy: 0.9537\n",
      "Epoch 369/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1136 - binary_accuracy: 0.9505\n",
      "Epoch 370/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1227 - binary_accuracy: 0.9509\n",
      "Epoch 371/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1123 - binary_accuracy: 0.9517\n",
      "Epoch 372/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1052 - binary_accuracy: 0.9523\n",
      "Epoch 373/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1196 - binary_accuracy: 0.9492\n",
      "Epoch 374/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1132 - binary_accuracy: 0.9517\n",
      "Epoch 375/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1059 - binary_accuracy: 0.9551\n",
      "Epoch 376/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1055 - binary_accuracy: 0.9572\n",
      "Epoch 377/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1160 - binary_accuracy: 0.9525\n",
      "Epoch 378/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1372 - binary_accuracy: 0.9445\n",
      "Epoch 379/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1063 - binary_accuracy: 0.9554\n",
      "Epoch 380/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1164 - binary_accuracy: 0.9529\n",
      "Epoch 381/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1104 - binary_accuracy: 0.9537\n",
      "Epoch 382/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1057 - binary_accuracy: 0.9558\n",
      "Epoch 383/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1079 - binary_accuracy: 0.9535\n",
      "Epoch 384/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1152 - binary_accuracy: 0.9539\n",
      "Epoch 385/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1091 - binary_accuracy: 0.9521\n",
      "Epoch 386/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0996 - binary_accuracy: 0.9576\n",
      "Epoch 387/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1036 - binary_accuracy: 0.9566\n",
      "Epoch 388/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1053 - binary_accuracy: 0.9535\n",
      "Epoch 389/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1208 - binary_accuracy: 0.9513\n",
      "Epoch 390/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1049 - binary_accuracy: 0.9558\n",
      "Epoch 391/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1041 - binary_accuracy: 0.9566\n",
      "Epoch 392/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1138 - binary_accuracy: 0.9519\n",
      "Epoch 393/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1043 - binary_accuracy: 0.9549\n",
      "Epoch 394/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1401 - binary_accuracy: 0.9452\n",
      "Epoch 395/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1026 - binary_accuracy: 0.9570\n",
      "Epoch 396/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1031 - binary_accuracy: 0.9572\n",
      "Epoch 397/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1079 - binary_accuracy: 0.9568\n",
      "Epoch 398/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1108 - binary_accuracy: 0.9509\n",
      "Epoch 399/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1026 - binary_accuracy: 0.9596\n",
      "Epoch 400/400\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1134 - binary_accuracy: 0.9511\n"
     ]
    }
   ],
   "source": [
    "offset = 0\n",
    "kernel_size = 3\n",
    "pooling_size = 2\n",
    "lr = 0.001\n",
    "epochs = 400\n",
    "dropout = False\n",
    "layers_repeat = 2\n",
    "frame_len = 10\n",
    "spike_threshold = 7\n",
    "layer_size = 128\n",
    "batch_size = 20\n",
    "\n",
    "cnn_data = create_cnn_data(data_train, frame_len=frame_len, offset=offset, spike_threshold=spike_threshold)\n",
    "X = np.array([cnn_data.x[1]])\n",
    "for x in cnn_data.x[1:]:\n",
    "    X = np.concatenate((X, [x]), axis=0)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "model = create_model(layers_repeat, layers_repeat, kernel_size, pooling_size, \n",
    "                     frame_len=frame_len, layer_size=layer_size, dropout=dropout)\n",
    "model.fit(X, cnn_data.y, epochs=epochs, batch_size=batch_size)\n",
    "ba, f1 = evaluate_cnn(test, model, frame_len=frame_len, offset=offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0733 - binary_accuracy: 0.9692\n",
      "Epoch 2/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0682 - binary_accuracy: 0.9721\n",
      "Epoch 3/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0564 - binary_accuracy: 0.9782\n",
      "Epoch 4/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0765 - binary_accuracy: 0.9678\n",
      "Epoch 5/100\n",
      "246/246 [==============================] - 1s 2ms/step - loss: 0.0692 - binary_accuracy: 0.9715\n",
      "Epoch 6/100\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.0622 - binary_accuracy: 0.9743\n",
      "Epoch 7/100\n",
      "246/246 [==============================] - 1s 2ms/step - loss: 0.0656 - binary_accuracy: 0.9747\n",
      "Epoch 8/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0851 - binary_accuracy: 0.9674\n",
      "Epoch 9/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0616 - binary_accuracy: 0.9764\n",
      "Epoch 10/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0684 - binary_accuracy: 0.9729\n",
      "Epoch 11/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0633 - binary_accuracy: 0.9727\n",
      "Epoch 12/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0700 - binary_accuracy: 0.9723\n",
      "Epoch 13/100\n",
      "246/246 [==============================] - 1s 2ms/step - loss: 0.2120 - binary_accuracy: 0.9333\n",
      "Epoch 14/100\n",
      "246/246 [==============================] - 1s 2ms/step - loss: 0.1057 - binary_accuracy: 0.9558\n",
      "Epoch 15/100\n",
      "246/246 [==============================] - 1s 2ms/step - loss: 0.0730 - binary_accuracy: 0.9706\n",
      "Epoch 16/100\n",
      "246/246 [==============================] - 1s 2ms/step - loss: 0.0579 - binary_accuracy: 0.9768\n",
      "Epoch 17/100\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.0561 - binary_accuracy: 0.9772\n",
      "Epoch 18/100\n",
      "246/246 [==============================] - 1s 2ms/step - loss: 0.0695 - binary_accuracy: 0.9684\n",
      "Epoch 19/100\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.0564 - binary_accuracy: 0.9759\n",
      "Epoch 20/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0604 - binary_accuracy: 0.9741\n",
      "Epoch 21/100\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.0829 - binary_accuracy: 0.9674\n",
      "Epoch 22/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0788 - binary_accuracy: 0.9710\n",
      "Epoch 23/100\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.0755 - binary_accuracy: 0.9719\n",
      "Epoch 24/100\n",
      "246/246 [==============================] - 1s 2ms/step - loss: 0.0707 - binary_accuracy: 0.9715\n",
      "Epoch 25/100\n",
      "246/246 [==============================] - 1s 2ms/step - loss: 0.0840 - binary_accuracy: 0.9668\n",
      "Epoch 26/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0782 - binary_accuracy: 0.9664\n",
      "Epoch 27/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1266 - binary_accuracy: 0.9533\n",
      "Epoch 28/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0636 - binary_accuracy: 0.9743\n",
      "Epoch 29/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0595 - binary_accuracy: 0.9731\n",
      "Epoch 30/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0666 - binary_accuracy: 0.9733\n",
      "Epoch 31/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0551 - binary_accuracy: 0.9770\n",
      "Epoch 32/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0561 - binary_accuracy: 0.9759\n",
      "Epoch 33/100\n",
      "246/246 [==============================] - 1s 2ms/step - loss: 0.0623 - binary_accuracy: 0.9723\n",
      "Epoch 34/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0646 - binary_accuracy: 0.9741\n",
      "Epoch 35/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0681 - binary_accuracy: 0.9706\n",
      "Epoch 36/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0687 - binary_accuracy: 0.9727\n",
      "Epoch 37/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0761 - binary_accuracy: 0.9715\n",
      "Epoch 38/100\n",
      "246/246 [==============================] - 1s 2ms/step - loss: 0.0730 - binary_accuracy: 0.9704\n",
      "Epoch 39/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0662 - binary_accuracy: 0.9755\n",
      "Epoch 40/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0643 - binary_accuracy: 0.9755\n",
      "Epoch 41/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.2285 - binary_accuracy: 0.9427\n",
      "Epoch 42/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1103 - binary_accuracy: 0.9572\n",
      "Epoch 43/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0618 - binary_accuracy: 0.9733\n",
      "Epoch 44/100\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.0539 - binary_accuracy: 0.9808\n",
      "Epoch 45/100\n",
      "246/246 [==============================] - 1s 2ms/step - loss: 0.0481 - binary_accuracy: 0.9808\n",
      "Epoch 46/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0510 - binary_accuracy: 0.9798\n",
      "Epoch 47/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0743 - binary_accuracy: 0.9739\n",
      "Epoch 48/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0726 - binary_accuracy: 0.9686\n",
      "Epoch 49/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0591 - binary_accuracy: 0.9757\n",
      "Epoch 50/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0557 - binary_accuracy: 0.9766\n",
      "Epoch 51/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0652 - binary_accuracy: 0.9751\n",
      "Epoch 52/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0617 - binary_accuracy: 0.9741\n",
      "Epoch 53/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0674 - binary_accuracy: 0.9725\n",
      "Epoch 54/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0630 - binary_accuracy: 0.9743\n",
      "Epoch 55/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0615 - binary_accuracy: 0.9755\n",
      "Epoch 56/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0561 - binary_accuracy: 0.9782\n",
      "Epoch 57/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0592 - binary_accuracy: 0.9735\n",
      "Epoch 58/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0600 - binary_accuracy: 0.9759\n",
      "Epoch 59/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0600 - binary_accuracy: 0.9747\n",
      "Epoch 60/100\n",
      "246/246 [==============================] - 1s 2ms/step - loss: 0.0919 - binary_accuracy: 0.9672\n",
      "Epoch 61/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1043 - binary_accuracy: 0.9600\n",
      "Epoch 62/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0632 - binary_accuracy: 0.9743\n",
      "Epoch 63/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0584 - binary_accuracy: 0.9772\n",
      "Epoch 64/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0878 - binary_accuracy: 0.9670\n",
      "Epoch 65/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0809 - binary_accuracy: 0.9686\n",
      "Epoch 66/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0521 - binary_accuracy: 0.9794\n",
      "Epoch 67/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0462 - binary_accuracy: 0.9798\n",
      "Epoch 68/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0581 - binary_accuracy: 0.9774\n",
      "Epoch 69/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0558 - binary_accuracy: 0.9778\n",
      "Epoch 70/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1135 - binary_accuracy: 0.9619\n",
      "Epoch 71/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1037 - binary_accuracy: 0.9602\n",
      "Epoch 72/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0622 - binary_accuracy: 0.9745\n",
      "Epoch 73/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0532 - binary_accuracy: 0.9776\n",
      "Epoch 74/100\n",
      "246/246 [==============================] - 1s 2ms/step - loss: 0.0454 - binary_accuracy: 0.9817\n",
      "Epoch 75/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0512 - binary_accuracy: 0.9800\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 1s 2ms/step - loss: 0.0484 - binary_accuracy: 0.9810\n",
      "Epoch 77/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0620 - binary_accuracy: 0.9764\n",
      "Epoch 78/100\n",
      "246/246 [==============================] - 1s 2ms/step - loss: 0.0931 - binary_accuracy: 0.9655\n",
      "Epoch 79/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0827 - binary_accuracy: 0.9672\n",
      "Epoch 80/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0481 - binary_accuracy: 0.9792\n",
      "Epoch 81/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0523 - binary_accuracy: 0.9786\n",
      "Epoch 82/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0617 - binary_accuracy: 0.9766\n",
      "Epoch 83/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0735 - binary_accuracy: 0.9708\n",
      "Epoch 84/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0582 - binary_accuracy: 0.9747\n",
      "Epoch 85/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0592 - binary_accuracy: 0.9770\n",
      "Epoch 86/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0687 - binary_accuracy: 0.9739\n",
      "Epoch 87/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0505 - binary_accuracy: 0.9788\n",
      "Epoch 88/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0646 - binary_accuracy: 0.9735\n",
      "Epoch 89/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0535 - binary_accuracy: 0.9786\n",
      "Epoch 90/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0867 - binary_accuracy: 0.9645\n",
      "Epoch 91/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0823 - binary_accuracy: 0.9690\n",
      "Epoch 92/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0616 - binary_accuracy: 0.9755\n",
      "Epoch 93/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0834 - binary_accuracy: 0.9686\n",
      "Epoch 94/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0632 - binary_accuracy: 0.9766\n",
      "Epoch 95/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0511 - binary_accuracy: 0.9806\n",
      "Epoch 96/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0474 - binary_accuracy: 0.9800\n",
      "Epoch 97/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0605 - binary_accuracy: 0.9766\n",
      "Epoch 98/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.1169 - binary_accuracy: 0.9623\n",
      "Epoch 99/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0622 - binary_accuracy: 0.9745\n",
      "Epoch 100/100\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 0.0541 - binary_accuracy: 0.9812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ba2189e6d0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, cnn_data.y, epochs=100, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_results(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
